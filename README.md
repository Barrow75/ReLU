# ReLU
The Rectified Linear Activation Function

- A piecewise linear function that introduces non linearity to the network opperations
  enabling it to learn complex relationships in the data

*Primary Advantages*  
  - Non-Linearity: Allows the neural networks to model more complex relationships in the data
  - Computationally Efficeient: Very effeicent to compute and differentiate. Making it a popular choice in deep learning
  - Mitigating Vanishing Graident Problem: Helps mitigate the vanishinggradient problem (sigmoid, tanh etc) making it more feasible 
